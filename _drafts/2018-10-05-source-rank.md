---
layout:     post
title:      “SourceRank.org - Open-source quality assessment of news media publications“
date:       2018-16-02
summary:  Our Facebook Hack 2018 project
categories: software-engineering fake-news news-media
---

![INCLUDE THE PICTURE]()

For the 2018 [Facebook Melbourne Hackathon](https://www.facebook.com/events/432440330543435/), me and three other RMIT students ([Callan](https://github.com/zcallan), [Matt](https://github.com/matthaywardwebdesign), and Michael) built the sourcerank.org platform, and MVP implementation of an open tool for news and information quality evaluation. At presentation time, our platform included a web app (www.sourcerank.org), a [basic Facebook Messenger bot](https://www.messenger.com/t/166585247361466), a Chrome extension, and a public api (www.sourcerank.org/api/). Though we didn’t place, probably due to a bungled presentation, I think the idea has merit and the design and implementation was interesting enough to warrant further description. The following I hope gives a clear understanding of the thinking behind sourcerank.org, the immediate utility of our polished up ‘source rank 0.2’ implementation, and of a longer term vision for open-source, community-driven information quality evaluation tooling.

### The Problem
‘Fake News’ has become a pretty prominent problem in the media lately, but more broadly, social media has become a powerful vector for low-quality and even malicious media that while not outright ‘fake’ or ‘false’, is a burden on the information systems it finds itself in.

Because of it’s significance in the 2016 US election, public and political consciousness around this issue has been raised and thus the giant tech companies like Facebook and Google has initiated or ramped up their efforts to maintain ‘information quality’ on their platforms. We included [this quote by Mark Zuckerberg](https://clyp.it/ugvvtt1e#) in our presentation, taken from [his April appearance on The Ezra Klein podcast](https://art19.com/shows/the-ezra-klein-show/episodes/0d5f503d-80d0-4e98-aa08-d29599957459). In it Zuckerberg details efforts to survey their user base in order to discover which new sources were broadly understood by their users to be trustworthy. The motivation here is to establish a ‘common ground’ and ‘shared understanding’ between polarised peoples. If Facebook can get a signal on what in untrustworthy and ban it, and also get a signal on what their users are willing to agree is credible, they get a better information system. I think this is a great goal, and one that I wanted to tackle, but from a different direction.

### The Solution
The **major** problem that I see with Facebook’s approach to the problem is that it is internal and inaccessible. When Facebook do these surveys, they don’t publish their methodology and they don’t publish their results. Facebook, and Google, are regularly criticised for keeping their approaches to platform content maintenance and censorship private. Here’s [one recent example article](https://techcrunch.com/2018/05/07/santa-clara-principles-eff-cdt-aclu-facebook-google-twitter/). The reasons for the criticism are clear. Without an open-approach they lack accountability, oversight, and *user involvement*. Facebook obviously sees the value in involving users in part of the process (they surveyed them), but it would be better if the public were involved in the *whole process*, the entire overall endeavour of cleaning up our information systems.

Noting this problem, our solution, sourcerank.org, is designed to be open-source, open-methodology, open to feedback, and open to user  configuration.

The second clear problem with Facebook’s approach of surveying users for their opinions on news trustworthiness is an important one, but one that perhaps sits uncomfortably with Facebook (and Google) for a couple of reasons. It is the problem that users may have *bad reasons for trusting a news media site*. In audio clip linked up above, Zuckerberg calls out *The New York Times*, and *The Wall Street Journal* as two site that are ‘broadly trusted’, even by those that don’t typically agree with those publication’s content. The mentioning of those names is assuring, but given their methodology it is entirely possible that ridiculous sites like *The Daily Stormer* and *Info Wars* may have come up as most trustworthy amongst users. We could certainly imagine that happening if this kind of surveying was conducted on the alt-right-wing platform [voat.com](https://voat.co/). Would Facebook have been happy to run with their results if their users had communicated to them that *Breitbart.com* was unassailably trustworthy and that *The New York Times* was a “traitorous (((globalist))) rag”? It is better, I believe, to have userbases settle on *why* publications are trustworthy, *what makes them trustworthy*, rather than merely on which publications are trustworthy.

The interest than centers on the features of good information. We intuit that what constitutes good information, and a trustworthy news publication is an incredibly complicated question to answer. There are dozens of ‘signals’ that contribute to an assessment in a person’s mind, and layers and layers of nuance. How can we build computer systems that, while recognising the undoubted quality of a publication like *The New York Times*, also consider that publication’s place in Chomsky’s propaganda theory of media outlined in [*Manufacturing Consent*](https://en.wikipedia.org/wiki/Manufacturing_Consent)? The short answer would be that we can’t yet, but we can I think make a start. A number of these ‘signals’ can be picked out and ‘computerised’, creating an automated system of information quality monitoring and evaluation. Another way of speaking of these ‘signals’ is as ‘heuristics’.  Our team’s platform, sourcerank.org, is basically an open system for the conglomeration of algorithmic news-quality heuristics that assess whether a *publication* is good, a system in the same family as one described [this article by Stanford John S. Knight Fellow, Frederic Filloux](https://mondaynote.com/dealing-with-an-elusive-corpus-of-news-stories-ea039f9db914), for evaluating the content of *specific articles*.
